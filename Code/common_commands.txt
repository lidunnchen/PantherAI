#"SuppDoc1_PracticalNotes_Guide" attached in the supplemental materials provides some practical notes to get started. 

## Upon opening a new Anaconda Prompt window at the start of a session, you will need to set your environment and specify your project directory (the project folder where all you project files, including images and labels, are located. You can do his by using the "conda activate" and "cd" commands as shown below. Then, your environment should be set up to begin training or deploying models. 

### All commands and scripts provided below were developed and run in Anaconda Prompt but can be adapted to other environments. 

#### Refer to the associated article (Chen et al., 2025) and Supplemental Table 1 for descriptions of each script's intended usage and parameters.  


conda activate TT # opens the working environment with the required software dependencies; refer to “Supplemental Document 1” for more details on how to set up a environment 
## Replace TT with the name of your Anaconda environment

cd C:\Users\lchen\Desktop\master_combined_compiled #specifies the project folder where video frames and labels are contained, along with the .yaml file brought in from the video annotation software 

### TRAINING YOLOV8 MODEL		

yolo task=detect mode=train epochs=200 data=data_custom.yaml model= yolov8m.pt imgsz=640 batch=45 patience=10 device=0 plots=True augment=False workers = 6

# If not using full GPU memory, increase batch size... batch=64 brings us to full use of graphics card (23.2/24 GBs utilized)

### DEPLOYING YOLOV8 MODEL

yolo task=detect mode=val model=best.pt data=data_custom.yaml conf=0.6 classes=0,1,2,3,4
	# for deploying the trained model on an internal or external validation dataset
	# the "runs" folder that is generated following this command includes the confusion matrix and other important metrics


###model testing on full continuous video sequence
## make sure set cd properly and consider placing all videos of interest for testing in a folder: cd C:\Users\lchen\Desktop\addTestData\VIDS4Testing 

yolo task=detect mode=predict model=best_balanced_noAug.pt source="C:\Users\lchen\Desktop\addTestData\VIDS4Testing\test\C28_9_25_2024 10_08_10 PM (UTC-04_00).mkv" conf=0.6 show=True save=True save_txt=True

	# mode = predict  ; different from val or train option!
 	# show = True     ; for immediate visual feedback during development or testing 
	# conf =     > increase to include only predictions that the model is confident in placing; can help to decrease false positives

		
### Once model has been validated and performing well, gain behavioural insights by automatically generating activity budgets and space use heatmaps.

python 7_activity_budget.py 
# this provided script will conduct automated behavoiural inference and output the number of detections, % that each behavior is engaged in for the duration of the video, both in a table .csv format as well as a barplot.
# you will need to edit the script in Notepad prior to running and ensure the file paths are accurate, particularly for the video(s) you want to analyze.

python 8_dynamic_ethogram.py 
# this provided script will apply automated behavioural inference and output a dynamically changing ethogram to the video you specify; again, you must be sure to link the script to point to the correct file path for the video you wish to analyze.

python 9_heatmap_video.py
# this script applies the YOLOv8 model to conduct automated behavioural inference and outputs a heatmap based off the magnitude of detections across the video map.
# if you are evaluating behaviours across multiple behaviours or habitats, you must be careful to provide the correct directory and analyze videos that are belonging to the same habitat altogether rather than analyzing videos from multiple habitats with different layouts/image dimensions.
# for instance, in our manuscript, we had three camera angles for separate habitat areas. When applying this script, we grouped videos into separate folders for analysis, based off the camera angle/habitat location. 

python 1-_video_inference.py
# this script takes a raw video and applies automated behavioural inference. The output will be the raw video with bounding box and behavioural class predictions overlayed.  


